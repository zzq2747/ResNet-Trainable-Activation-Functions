# ResNet-Trainable-Activation-Functions

This repository contains all the figures and tables associated with the paper titled "Web-Aided Dataset Expansion in Deep Learning: Evaluating Trainable Activation Functions in ResNet for Improved Image Classification."

## Abstract

This paper explores the impact of integrating trainable activation functions—namely, CosLU, DELU, and ReLUN—into ResNet architectures. The study utilizes CIFAR-10 and CIFAR-100 datasets to conduct a comparative analysis, revealing that trainable non-linearities can significantly enhance the performance of image classification models.

## Figures

The figures included in this repository represent the various experimental results and comparative analyses conducted throughout the research. Each figure is a product of extensive data processing and experimentation.

- **Figure 1**: ResNet-n model network architecture.
- **Figure 2**: Residual learning: a building block.
- **Figure 3**: Examples of ResNet-n (n=8,14,20,26,32) Network Architectures. Dotted lines indicate a dimension increase, and the same color represents the same dimension (using the CIFAR-100 dataset for training).
- **Figure 4**: A comparison of activation functions on training the ResNet-8 model on the CIFAR-10 dataset.
- **Figure 5**: A comparison of activation functions on training the ResNet-14 model on the CIFAR-10 dataset.
- **Figure 6**: A comparison of activation functions on training the ResNet-20 model on the CIFAR-10 dataset.
- **Figure 7**: A comparison of activation functions on training the ResNet-26 model on the CIFAR-10 dataset.
- **Figure 8**: A comparison of activation functions on training the ResNet-32 model on the CIFAR-10 dataset.
- **Figure 9**: A comparison of activation functions on training the ResNet-8 model on the CIFAR-100 dataset. 
- **Figure 10**: A comparison of activation functions on training the ResNet-14 model on the CIFAR-100 dataset. 
- **Figure 11**: A comparison of activation functions on training the ResNet-20 model on the CIFAR-100 dataset. 
- **Figure 12**: A comparison of activation functions on training the ResNet-26 model on the CIFAR-100 dataset. 
- **Figure 13**: A comparison of activation functions on training the ResNet-32 model on the CIFAR-100 dataset. 


## Tables

The tables provide a detailed breakdown of the experimental setups, results, and comparative performance metrics.

- **Table I**: Basic Architectures of ResNet-n (n=8,14,20,26,32) Models (using CIFAR-10 dataset for training).
- **Table II**: Comparison of CIFAR-10 and CIFAR-100 Datasets.
- **Table III**: Global hyperparameters settings for the ResNet-n models in the experiments.
- **Table  IV**: Comparison of accuracy (%) on the CIFAR-10 dataset for ResNet-n (n=8,14,20,26,32) models (training/testing).
- **Table V**: Comparison of accuracy (%) on the CIFAR-100 dataset for ResNet-n (n=8,14,20,26,32) models (training/testing).

## Ownership and Rights

All figures and tables presented in this repository are original and were created by the author(s) of the associated paper. They are intended to supplement the findings and discussions presented in the paper.


## Contact

For any inquiries or further discussion regarding the research, figures, or tables, please reach out to ZhangZhiqing at 222309252018@zust.edu.cn.

---

This repository is maintained by ZhangZhiqing. © 2024. All rights reserved.
